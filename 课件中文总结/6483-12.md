---
      
title: 12
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

# 人工智能与数据挖掘 - 第12周学习文档


## 第一部分：模型复杂性与降维

### 1.1 偏差与方差回顾
在机器学习中，模型的复杂性直接影响其表现。以下是对偏差（Bias）和方差（Variance）的简要回顾：

- **偏差（Bias）**：模型由于过于简单而无法捕捉数据中的复杂关系，导致预测结果系统性地偏离真实值。
- **方差（Variance）**：模型由于过于复杂而过度拟合训练数据中的噪声，导致对新数据的预测不稳定。

#### 欠拟合（Underfitting）
- **特征**：高偏差，低方差
- **原因**：模型过于简单，未能捕捉输入特征与目标输出之间的关键关系。
- **表现**：训练误差和测试误差均较高。

#### 过拟合（Overfitting）
- **特征**：低偏差，高方差
- **原因**：模型过于复杂，捕捉了训练数据中的噪声而非真实模式。
- **表现**：训练误差很低，但测试误差较高（泛化能力差）。

#### 过拟合与欠拟合的关系
- 高复杂度的模型容易导致过拟合，训练与测试误差之间差距较大。
- 低复杂度的模型容易导致欠拟合，训练与测试误差之间差距较小，但整体误差较高。

#### 示例问题（PPT中提供）
假设您正在训练一个用于分类的神经网络（NN）：
1. 当您通过移除一层来降低模型复杂性时，验证准确率提高了。
   - **模型的偏差与方差如何变化？**  
     答：移除层降低了模型复杂性，偏差增加，方差减少。
   - **模型是过拟合还是欠拟合？**  
     答：模型之前可能过拟合，因为降低复杂性后验证准确率提高。
2. 如果您使神经网络更深（增加复杂性）：
   - **训练准确率会提高吗？**  
     答：可能会提高，因为更复杂的模型能更好地拟合训练数据。
   - **验证准确率会提高吗？**  
     答：不一定，复杂性增加可能导致过拟合，验证准确率可能下降。

### 1.2 防止过拟合的正则化方法
为了缓解过拟合问题，可以采取以下策略（以神经网络为例）：
1. **限制模型复杂性**：减少模型的表达能力（如减少层数或神经元数量）。
2. **增加训练数据的复杂性/规模**：通过扩充数据集或数据增强，降低方差。
3. **简化数据分布和维度**：通过降维技术减少特征冗余和噪声。

其中，**降维（Dimensionality Reduction）** 是一种关键技术，用于简化高维数据，防止过拟合，并提高算法效率。

---

## 第二部分：降维与主成分分析（PCA）

### 2.1 降维的概念
**降维** 是将高维数据转换为低维数据的技术，目标是：
- 在尽可能少的信息损失下，找到一个最适合的低维子空间来表示原始数据。
- 例如：将二维数据拟合到一维直线，或将三维数据拟合到二维平面。

#### 降维的必要性
高维数据（即特征数量多）会带来以下问题：
- **计算复杂度高**：处理高维数据需要更多计算资源，算法效率低。
- **过拟合风险**：特征过多容易导致模型拟合噪声或数据损坏。
- **可视化困难**：高维数据难以直观展示。

#### 降维的好处
- **去除特征冗余与噪声**：简化数据分布。
- **防止过拟合**：降低模型复杂性。
- **提高可视化效果**：便于数据分析和展示。

#### 降维示例
1. **二维到一维**：将二维数据（例如身高以英寸和厘米表示）投影到一维直线，减少冗余。
2. **三维到二维**：将三维数据点投影到二维平面上，用更简单的表示保留主要信息。

### 2.2 主成分分析（PCA）
**主成分分析（PCA）** 是一种简单而流行的无监督学习方法，用于降维。其目标是找到数据的最佳低维子空间进行投影。

#### PCA的“最佳”定义
PCA通过以下两个目标来定义“最佳”子空间：
1. **最小化投影数据的均方误差（MSE）**：使原始数据到投影子空间的垂直偏移（误差）最小。
2. **最大化投影数据的方差**：使投影后的数据尽可能分散，保留最多的信息。

**重要结论**：最小化MSE与最大化投影方差是等价的。
- 数学依据：对于给定数据点，原始数据（黑色向量）的平方 = 投影数据（蓝色向量）的平方 + 误差（绿色向量）的平方。
- 由于原始数据是固定的，最大化蓝色部分（投影方差）等同于最小化绿色部分（MSE）。

#### PCA的直观解释
- **最小化MSE**：找到一个子空间，使得数据点到该子空间的垂直距离平方和最小。
- **最大化方差**：找到一个子空间，使得数据点投影后在该子空间上的分布尽可能广泛。

#### PCA示例
1. **投影方差比较**：在不同子空间方向（向量v）上，投影方差较大的方向更能代表数据的主要变化。
2. **投影MSE比较**：投影误差（垂直偏移）较小的方向是更好的子空间选择。

### 2.3 PCA的推导（可选，非考试内容）
以下是PCA的数学推导过程，帮助理解其原理。

#### 目标1：最小化MSE
- 对于数据点集合 {x_i}（i=1到N），找到单位向量v，使得投影后的MSE最小。
- 投影点：z_i = v^T x_i v
- MSE公式：  
  MSE = (1/N) Σ (z_i - x_i)^2
- 目标是找到v，使得MSE最小化。

#### 目标2：最大化投影方差
- 对于零均值数据 {x_i}，投影数据的方差定义为：  
  Var = (1/N) Σ (z_i)^2 = (1/N) Σ (v^T x_i)^2
- 目标是找到v，使得Var最大化，同时v是单位向量（||v||_2 = 1）。

#### 数学问题形式化
- PCA的目标是最大化投影方差：  
  max_{||v||_2=1} (1/N) Σ (v^T x_i)^2 = max_{||v||_2=1} (1/N) v^T X^T X v
- 其中，X 是数据矩阵，行表示数据点，列表示特征维度。

#### 解决方案：奇异值分解（SVD）
- 对数据矩阵X应用SVD分解：X = U Σ V^T
- V = [v_1, v_2, ..., v_d]，其中v_k 是第k个主成分的单位权重向量。
- 第k个主成分：z_{i,k} = (v_k^T x_i) v_k
- 投影到k维子空间：V_k^T x_i，其中V_k = [v_1, ..., v_k]

#### 推导示例
给定数据点：x_1 = [4,6,10], x_2 = [3,10,13], x_3 = [-2,-6,-8]
1. 构建数据矩阵X：
   ```
   X = [ 4,  6, 10;
         3, 10, 13;
        -2, -6, -8]
   ```
2. 进行SVD分解：X = U Σ V^T
3. 得到V矩阵的第一列向量作为第一个主成分方向：v_1 = [-0.22, -0.57, -0.79]
4. 计算每个数据点的第一个主成分投影：z_{i,1} = (v_1^T x_i) v_1

### 2.4 PCA的应用示例
1. **手写数字识别（MNIST数据集）**
   - PCA用于提取手写数字图像的特征，提供更鲁棒和不变的表示。
   - 通过旋转、缩放等操作，PCA帮助保留主要特征。

2. **图像压缩**
   - **原始图像**：372x492像素，分割为12x12的图像块，每个块视为144维向量。
   - **PCA降维**：
     - 144D → 60D：保留大部分图像信息，压缩效果良好。
     - 144D → 16D：信息损失增加，但仍可辨识。
     - 144D → 6D 或 1D：信息损失严重，图像质量下降。
   - 显示了前16个最重要的特征向量（eigenvectors），用于重建图像。

---

## 第三部分：贝叶斯推理

### 3.1 从确定性到概率性学习
- **确定性模型**：对于相同输入，始终输出相同结果（如深度神经网络分类）。
- **概率性模型（贝叶斯推理）**：基于概率分布输出结果，考虑不确定性。
- **示例**：抛硬币两次，可能结果为 {HH, HT, TH, TT}，每个结果概率为0.25。

### 3.2 概率论基础概念
- **事件/假设（h）**：一个特定结果或命题，如抛硬币得到正面。
- **数据（D）**：观察到的信息，如抛硬币8次得到{H,H,H,H,T,T,T,T}。
- **概率符号**：
  - P(h)：假设h成立的概率（先验概率）。
  - P(D)：观察到数据D的概率。
  - P(D|h)：假设h成立时观察到D的概率（似然）。
  - P(h|D)：观察到D后假设h成立的概率（后验概率）。
- **目标**：计算P(h|D)，即基于已有数据D找到最可能的假设h。

### 3.3 联合概率与条件概率
- **条件概率 P(A|B)**：在B发生的情况下A发生的概率。
- **联合概率 P(A,B)**：A和B同时发生的概率。
- **关系**：P(A,B) = P(A|B) * P(B)

### 3.4 贝叶斯定理
- **基本形式**：  
  P(A|B) = [P(B|A) * P(A)] / P(B)
- **推导**：基于联合概率 P(A,B) = P(A|B)*P(B) = P(B|A)*P(A)。
- **应用形式**：  
  P(h|D) = [P(D|h) * P(h)] / P(D)
- **术语**：
  - 先验概率 P(h)：观察D前对h的信念。
  - 后验概率 P(h|D)：观察D后对h的更新信念。
  - 似然 P(D|h)：在h成立下观察D的可能性。
- **最大后验概率（MAP）**：  
  h_MAP = argmax P(h|D) = argmax P(D|h)*P(h)
- 若P(h)为常量，则MAP等同于最大似然（ML）：  
  h_MAP = argmax P(D|h)

### 3.5 朴素贝叶斯（Naïve Bayes）
- **问题**：当条件涉及多个属性时，计算条件概率变得复杂。
- **朴素贝叶斯假设**：假设各属性之间条件独立，即：  
  P(a_1, a_2, ..., a_d | v_j) = ∏ P(a_i | v_j)
- **应用**：将复杂的多条件概率分解为单个条件的乘积，简化计算。

#### 示例：玩网球决策
- **问题**：给定观察<Sunny, Cool, High, Strong>，是否玩网球？
- **数据表**：基于历史数据统计每个属性的条件概率。
- **计算**：
  1. P(yes | <S,C,H,S>) ∝ P(yes) * P(S|yes)*P(C|yes)*P(H|yes)*P(S|yes) = 9/14 * 0.00823 = 0.0051
  2. P(no | <S,C,H,S>) ∝ P(no) * P(S|no)*P(C|no)*P(H|no)*P(S|no) = 0.0207
- **结论**：P(yes | <S,C,H,S>) < P(no | <S,C,H,S>)，因此不玩网球。

---

## 第四部分：例题与答案
以下是PPT中提供的例题及其解答，供复习参考。

### 4.1 过拟合与欠拟合示例
- **问题**：假设训练一个分类神经网络，移除一层后验证准确率提高。
  - 模型偏差与方差变化？  
    **答**：偏差增加，方差减少。
  - 模型是过拟合还是欠拟合？  
    **答**：之前过拟合，降低复杂性后验证准确率提高。
- **问题**：如果增加网络深度：
  - 训练准确率是否提高？  
    **答**：可能提高，因为模型更复杂。
  - 验证准确率是否提高？  
    **答**：不一定，可能因过拟合而下降。

### 4.2 抛硬币概率问题
1. **Quiz 1**：抛硬币两次，两次均为正面的概率？  
   **答**：P(HH) = 0.5 * 0.5 = 0.25
2. **Quiz 2**：第一次为正面，第二次也为正面的概率？  
   **答**：P(H2|H1) = 0.5（两次抛硬币独立）。

### 4.3 学生考试通过率问题
- **问题**：EEE学生占60%，通过率30%；IEM学生占40%，通过率50%。通过考试的学生中IEM学生占比？  
  **答**：  
  P(IEM|Pass) = [P(Pass|IEM)*P(IEM)] / P(Pass)  
  P(Pass) = P(Pass|EEE)*P(EEE) + P(Pass|IEM)*P(IEM) = 0.3*0.6 + 0.5*0.4 = 0.38  
  P(IEM|Pass) = (0.5*0.4) / 0.38 ≈ 0.526（52.6%）

### 4.4 Monty Hall问题
- **问题**：游戏中有三扇门，一扇有奖品。选手选一扇，主持人打开另一扇无奖品的门，是否应换门？  
  **答**：应该换门。初始选对概率为1/3，换门后概率提高到2/3。（详见概率分析或维基百科：Monty Hall Problem）

### 4.5 玩网球决策（Naïve Bayes）
- **问题**：观察<Sunny, Cool, High, Strong>，是否玩网球？  
  **答**：计算显示P(yes | obs) = 0.0051 < P(no | obs) = 0.0207，因此不玩。

---

## 第五部分：总结与关键问题
### 5.1 PCA关键问题
- **为何需要降维？**  
  答：去除冗余，防止过拟合，降低噪声，简化模型复杂性，提升可视化。
- **PCA的两个目标？**  
  答：最小化投影数据的均方误差（MSE），最大化投影数据的方差。
- **如何计算PCA？**  
  答：对数据矩阵应用SVD分解，选取V矩阵中的单位权重向量v_k，计算投影z_{i,k} = (v_k^T x_i) v_k。

### 5.2 贝叶斯推理关键问题
- **什么是贝叶斯定理？**  
  答：P(A|B) = [P(B|A)*P(A)] / P(B)，用于从先验和似然计算后验概率。
- **什么是朴素贝叶斯假设？**  
  答：假设各属性条件独立，即P(a_1,...,a_d | v_j) = ∏ P(a_i | v_j)。

---
