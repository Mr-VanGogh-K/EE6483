---
      
title: 7
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

## 1. 分类简介

**分类（Classification）** 是一项机器学习任务，根据对象的特征或属性将其分配到预定义的类别中。它通过一个函数 $ y = f(\mathbf{X}) $ 将输入对象（记为 $ \mathbf{X} $）映射到输出类别标签（$ y $）。

**应用场景**：
- 电子邮件分类（垃圾邮件与非垃圾邮件）。
- 语音识别。
- 图像分类。
- 信用卡交易欺诈检测（欺诈交易与合法交易）。

本课程重点介绍两种主要的分类方法：**最近邻分类器** 和 **支持向量机（SVM）**。

---

## 2. 最近邻分类器

### 2.1 基本概念
最近邻（Nearest Neighbor, NN）分类器将测试样本分配给距离最近的训练样本的类别标签。这种方法直观简单，但需要仔细考虑如何衡量“距离”或相似性。

- **训练阶段**：训练需求极低，仅需存储所有训练样本及其标签。
- **测试阶段**：计算量大，因为需要计算测试样本与所有训练样本之间的距离。

### 2.2 K-最近邻（K-NN）分类器
为了提高鲁棒性，尤其是在噪声数据或复杂分类问题中，K-最近邻（K-Nearest Neighbor, K-NN）方法考虑测试样本的 $ k $ 个最近训练样本的类别标签，并通过多数投票决定测试样本的类别。

- **优点**：通过考虑多个邻居减少对异常值的敏感性。
- **挑战**：$ k $ 值的选择会影响性能（小的 $ k $ 可能对噪声敏感；大的 $ k $ 可能导致决策边界过于平滑）。

**可视化**（PPT中内容）：
- 两个类别的训练样本（例如，红色代表类别0，蓝色代表类别1）。
- 对测试样本 $ \mathbf{x} $，NN分类器分配最近训练样本的标签，而K-NN分配 $ k $ 个最近样本中多数的标签。

### 2.3 相异性与相似性度量
为了确定“近邻”，需要衡量样本之间的相异性（距离）或相似性。

#### 2.3.1 闵可夫斯基距离（Minkowski Distance）
**闵可夫斯基距离** 是 $ n $ 维空间中两点 $ \mathbf{x} $ 和 $ \mathbf{y} $ 之间的广义距离度量，定义为：
$$
d(\mathbf{x}, \mathbf{y}) = \left( \sum_{k=1}^{n} |\mathbf{x}_k - \mathbf{y}_k|^r \right)^{1/r}
$$
其中 $ r $ 是一个参数，$ \mathbf{x}_k $ 和 $ \mathbf{y}_k $ 分别是 $ \mathbf{x} $ 和 $ \mathbf{y} $ 的第 $ k $ 个分量。

- **特殊情况**：
  - $ r = 1 $: **曼哈顿距离**（也称为城市街区距离或 $ L_1 $ 范数）：
    $$
    d(\mathbf{x}, \mathbf{y}) = \sum_{k=1}^{n} |\mathbf{x}_k - \mathbf{y}_k|
    $$
  - $ r = 2 $: **欧几里得距离**（最常用）：
    $$
    d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{k=1}^{n} (\mathbf{x}_k - \mathbf{y}_k)^2}
    $$
  - $ r \to \infty $: **上确界距离**（也称为 $ L_\infty $ 范数或切比雪夫距离），即对应属性之间的最大差异：
    $$
    d(\mathbf{x}, \mathbf{y}) = \max_{k} |\mathbf{x}_k - \mathbf{y}_k|
    $$

#### 2.3.2 度量性质
当 $ r \geq 1 $ 时，闵可夫斯基距离满足 **度量（Metric）** 的性质：
1. **非负性**：$ d(\mathbf{x}, \mathbf{y}) \geq 0 $，且 $ d(\mathbf{x}, \mathbf{y}) = 0 $ 仅当 $ \mathbf{x} = \mathbf{y} $ 时成立。
2. **对称性**：$ d(\mathbf{x}, \mathbf{y}) = d(\mathbf{y}, \mathbf{x}) $。
3. **三角不等式**：$ d(\mathbf{x}, \mathbf{z}) \leq d(\mathbf{x}, \mathbf{y}) + d(\mathbf{y}, \mathbf{z}) $。

某些距离度量可能不满足所有度量性质，但在特定场景中仍然有用。

#### 2.3.3 余弦相似性（Cosine Similarity）
**余弦相似性** 常用于衡量文档之间的相似性，文档通常表示为向量（例如，词频向量）。定义为：
$$
\cos(\mathbf{x}, \mathbf{y}) = \frac{\mathbf{x} \cdot \mathbf{y}}{|\mathbf{x}| |\mathbf{y}|} = \frac{\sum_{k=1}^{n} \mathbf{x}_k \mathbf{y}_k}{\sqrt{\sum_{k=1}^{n} \mathbf{x}_k^2} \sqrt{\sum_{k=1}^{n} \mathbf{y}_k^2}}
$$
- 它衡量向量 $ \mathbf{x} $ 和 $ \mathbf{y} $ 之间夹角 $ \theta $ 的余弦值。
- 值范围为-1（完全相反）到1（方向相同），0表示正交。

**应用示例**（PPT中内容）：
- 使用CIFAR-10数据集图像，通过像素级差异找到最近邻。

---

## 3. 支持向量机（SVM）

### 3.1 基本概念
**支持向量机（Support Vector Machines, SVMs）** 是一种监督机器学习模型，主要用于二分类任务。它们旨在找到一个超平面，将两类样本分隔开，并使超平面与最近样本之间的距离（称为边距）最大化。

- **二分类器**：使用超平面分隔两个类别（例如，2D中的线，3D中的平面，高维中的超平面）。
- **核心思想**：选择边距最大的超平面作为决策边界，以在未见过的数据上获得更好的泛化能力。

### 3.2 最大边距超平面（Maximum Margin Hyperplane）
对于可分数据，SVM确定 **最大边距超平面** 作为决策边界。

- 在多个可能分隔类别的超平面中，SVM选择与两类最近数据点之间距离最大的超平面。
- 与超平面最近的数据点称为 **支持向量（Support Vectors）**，边距是平行于决策边界并穿过支持向量的两个超平面之间的距离。

**数学表述**：
- 给定 $ N $ 个训练样本 $ (\mathbf{x}_i, y_i) $，其中 $ y_i \in \{-1, 1\} $ 是类别标签，决策边界为超平面：
  $$
  \mathbf{w}^T \mathbf{x} + b = 0
  $$
  其中 $ \mathbf{w} $ 和 $ b $ 是模型参数。
- 样本 $ \mathbf{x}_i $ 的类别标签预测为：
  $$
  y_i = \text{sign}(\mathbf{w}^T \mathbf{x}_i + b)
  $$
- 平行于决策边界并通过支持向量的超平面为：
  $$
  \mathbf{w}^T \mathbf{x} + b = 1 \quad \text{和} \quad \mathbf{w}^T \mathbf{x} + b = -1
  $$
- 边距 $ d $ 计算为：
  $$
  d = \frac{2}{|\mathbf{w}|}
  $$
- **目标**：最大化边距 $ d $，等价于最小化 $ |\mathbf{w}|^2/2 $：
  $$
  \min \frac{1}{2} |\mathbf{w}|^2
  $$
  约束条件为：
  $$
  y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1, \quad i = 1, 2, ..., N
  $$

**优化方法**：
- 上述问题使用 **拉格朗日乘子法（Lagrange Multiplier Method）** 解决，转化为对偶优化问题：
  $$
  \max \sum_{i=1}^{N} \lambda_i - \frac{1}{2} \sum_{i,j=1}^{N} \lambda_i \lambda_j y_i y_j (\mathbf{x}_i \cdot \mathbf{x}_j)
  $$
  约束条件为 $ \lambda_i \geq 0 $ 且 $ \sum_{i=1}^{N} \lambda_i y_i = 0 $。
- Karush-Kuhn-Tucker (KKT) 条件确保只有支持向量（位于边距上的样本）的拉格朗日乘子 $ \lambda_i $ 非零。

**测试阶段**：对于测试样本 $ \mathbf{z} $，类别标签为：
$$
y = \text{sign}\left( \sum_{i=1}^{N} \lambda_i y_i (\mathbf{x}_i \cdot \mathbf{z}) + b \right)
$$

### 3.3 不可分情况下的SVM（软边距方法）
在现实数据中，类别可能无法完全分离。**软边距方法（Soft Margin Approach）** 通过引入松弛变量 $ \xi_i \geq 0 $ 允许一定的误分类：
- **松弛约束**：
  $$
  y_i (\mathbf{w}^T \mathbf{x}_i + b) \geq 1 - \xi_i, \quad i = 1, 2, ..., N
  $$
- **目标**：平衡边距宽度和分类误差：
  $$
  \min \frac{1}{2} |\mathbf{w}|^2 + C \sum_{i=1}^{N} \xi_i^k
  $$
  其中 $ C $ 是一个惩罚参数，控制边距宽度和分类误差之间的权衡，$ k $（通常设为1或2）定义惩罚形式。
- 该问题同样通过拉格朗日对偶优化求解，与可分情况类似。

### 3.4 多类别SVM
基本SVM是二分类器。对于多类别问题（有 $ k $ 个类别），常用两种策略：

1. **一对剩余（One-vs-Rest, OvR）**：
   - 训练 $ k $ 个二分类SVM，每个SVM将一个类别与剩余类别区分开。
   - 对测试样本，分配决策函数值最高的类别。
2. **一对一（One-vs-One, OvO）**：
   - 训练 $ k(k-1)/2 $ 个二分类SVM，每个SVM区分一对类别。
   - 对测试样本，分配在所有成对比较中获得最多投票的类别。

**工具**：常用库如 **LIBSVM** 用于实现SVM（https://www.csie.ntu.edu.tw/~cjlin/libsvm/）。

---

## 4. 关键概念总结

- **最近邻分类器**：基于最近训练样本分配类别标签，使用如闵可夫斯基距离的相异性度量或如余弦相似性的相似性度量。K-NN通过多数投票提高鲁棒性。
- **支持向量机（SVM）**：
  - 二分类器，通过超平面最大化类别之间的边距。
  - 通过软边距方法处理不可分数据，引入松弛变量。
  - 使用拉格朗日对偶和二次规划求解优化问题。
  - 通过OvR或OvO策略扩展到多类别分类。
- 两种方法都需要仔细选择参数（如K-NN的 $ k $，SVM的 $ C $）和距离/相似性度量以获得最佳性能。

---

## 5. 示例与练习题

以下是从PPT中提取的示例和练习题，并提供详细解答以供学习。

### 示例1：SVM决策边界计算
**问题**：给定训练样本：
- $ \mathbf{x}_1 = (1, 0), y_1 = -1 $
- $ \mathbf{x}_2 = (0, 1), y_2 = -1 $
- $ \mathbf{x}_3 = (0, -1), y_3 = -1 $
- $ \mathbf{x}_4 = (-1, 0), y_4 = -1 $
- $ \mathbf{x}_5 = (3, 1), y_5 = 1 $
- $ \mathbf{x}_6 = (3, -1), y_6 = 1 $
- $ \mathbf{x}_7 = (4, 2), y_7 = 1 $
- $ \mathbf{x}_8 = (4, -2), y_8 = 1 $

假设 $ \mathbf{x}_3, \mathbf{x}_5, \mathbf{x}_6 $ 是支持向量，计算决策边界和SVM分类器。

**解答**（参考PPT第27-28页）：
- 因为 $ \mathbf{x}_3, \mathbf{x}_5, \mathbf{x}_6 $ 是支持向量，它们的拉格朗日乘子 $ \lambda_3, \lambda_5, \lambda_6 \neq 0 $，且类别标签为 $ y_3 = -1, y_5 = 1, y_6 = 1 $。
- 根据对偶优化约束和KKT条件：
  - $ \mathbf{w} = \sum \lambda_i y_i \mathbf{x}_i = -\lambda_3 \mathbf{x}_3 + \lambda_5 \mathbf{x}_5 + \lambda_6 \mathbf{x}_6 = -\lambda_3 (0, -1) + \lambda_5 (3, 1) + \lambda_6 (3, -1) $
  - 第一分量：$ w_1 = 3\lambda_5 + 3\lambda_6 $
  - 第二分量：$ w_2 = \lambda_3 + \lambda_5 - \lambda_6 $
  - 乘子之和：$ -\lambda_3 + \lambda_5 + \lambda_6 = 0 \Rightarrow \lambda_3 = \lambda_5 + \lambda_6 $
- 对支持向量应用约束：
  - 对于 $ \mathbf{x}_3 $: $ y_3 (\mathbf{w}^T \mathbf{x}_3 + b) = -1 \Rightarrow -(\lambda_3 (-1) + b) = -1 \Rightarrow -\lambda_3 + b = -1 $
  - 对于 $ \mathbf{x}_5 $: $ y_5 (\mathbf{w}^T \mathbf{x}_5 + b) = 1 \Rightarrow 3(3\lambda_5 + 3\lambda_6) + (1)(\lambda_3 + \lambda_5 - \lambda_6) + b = 1 $
    - 简化为：$ 9\lambda_5 + 9\lambda_6 + \lambda_3 + \lambda_5 - \lambda_6 + b = 1 \Rightarrow 10\lambda_5 + 8\lambda_6 + \lambda_3 + b = 1 $
  - 对于 $ \mathbf{x}_6 $: $ y_6 (\mathbf{w}^T \mathbf{x}_6 + b) = 1 \Rightarrow 3(3\lambda_5 + 3\lambda_6) + (-1)(\lambda_3 + \lambda_5 - \lambda_6) + b = 1 $
    - 简化为：$ 9\lambda_5 + 9\lambda_6 - \lambda_3 - \lambda_5 + \lambda_6 + b = 1 \Rightarrow 8\lambda_5 + 10\lambda_6 - \lambda_3 + b = 1 $
- 解方程组：
  - $ \lambda_3 = \lambda_5 + \lambda_6 $
  - $ -\lambda_3 + b = -1 $
  - $ 10\lambda_5 + 8\lambda_6 + \lambda_3 + b = 1 $
  - $ 8\lambda_5 + 10\lambda_6 - \lambda_3 + b = 1 $
- 最终解（PPT提供）：$ \lambda_3 = 1/2, \lambda_5 = 1/4, \lambda_6 = 1/4, b = -2 $
- 计算 $ \mathbf{w} $：
  - $ w_1 = 3(1/4) + 3(1/4) = 3/2 $
  - $ w_2 = (1/2) + (1/4) - (1/4) = 1/2 $
- 决策边界：$ \mathbf{w}^T \mathbf{x} + b = (3/2)x_1 + (1/2)x_2 - 2 = 0 \Rightarrow 3x_1 + x_2 - 4 = 0 $
- SVM分类器：$ y = \text{sign}(3x_1 + x_2 - 4) $

### 示例2：支持向量可行性检查
**问题**：使用相同数据，测试 $ \mathbf{x}_2 $ 和 $ \mathbf{x}_5 $ 是否可以作为支持向量。

**解答**（参考PPT第29页）：
- 假设 $ \lambda_2 \neq 0, \lambda_5 \neq 0 $，且 $ y_2 = -1, y_5 = 1 $。
- $ \mathbf{w} = -\lambda_2 \mathbf{x}_2 + \lambda_5 \mathbf{x}_5 = -\lambda_2 (0, 1) + \lambda_5 (3, 1) = (3\lambda_5, -\lambda_2 + \lambda_5) $
- 乘子之和：$ -\lambda_2 + \lambda_5 = 0 \Rightarrow \lambda_2 = \lambda_5 $
- 检查约束条件（PPT中）：计算得到的决策边界不满足其他点的约束（如 $ y_1 \mathbf{w}^T \mathbf{x}_1 + b \not\geq 1 $）。
- 结论：对于这些支持向量不可行。

---

## 6. 附加学习笔记
- **K-NN中选择 $ k $**：使用交叉验证选择 $ k $。优先选择奇数值以避免多数投票中的平局。
- **SVM参数 $ C $**：小的 $ C $ 优先考虑更大边距（可能导致更多错误）；大的 $ C $ 优先考虑更少错误（可能过拟合）。
- **SVM中的核技巧（Kernel Trick）**（PPT未涉及但相关）：对于非线性可分数据，SVM可通过核函数（如RBF、多项式）将数据映射到更高维空间，使其线性可分。
