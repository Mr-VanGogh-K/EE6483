---
      
title: 8
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

## 1. 神经网络简介

### 1.1 神经网络的基本概念
- **定义与灵感来源**：神经网络是一种模仿生物神经系统功能的计算模型，用于解决分类、回归等问题。人类大脑由约10^11个神经元组成，通过轴突和树突相互连接，连接点称为突触（约10^15个）。大脑通过反复刺激调整突触连接实现学习。
- **作用**：神经网络通过多层结构和非线性激活函数，能够近似任何分类或回归函数，具有通用逼近能力（Universal Approximation Theorem）。

### 1.2 关键术语
- **感知机（Perceptron）**：神经网络的基本单元，作为线性分类器，通过加权和与阈值比较输出0或1。
- **多层感知机（MLP）**：由输入层、隐藏层和输出层组成的前馈神经网络，能够逼近复杂函数。
- **激活函数（Activation Function）**：引入非线性，常见的包括Sigmoid、Tanh、ReLU等。
- **误差函数（Cost/Loss Function）**：衡量预测值与真实值差异的函数。
- **反向传播（Backpropagation）**：通过梯度下降调整权重以最小化误差的算法。
- **梯度下降（Gradient Descent）**：迭代优化算法，基于误差函数梯度更新权重。

---

## 2. 感知机（Perceptron）

### 2.1 模型结构
- 感知机接收多个输入$ x_1, x_2, ..., x_n $，计算加权和$ X = \sum_{i=1}^n w_i x_i + b $，通过激活函数$ \sigma(X) $输出类别（0或1）。
- 数学表达：
  $$
  y = \sigma\left(\sum_{i=1}^n w_i x_i + b\right)
  $$
  其中，$ \sigma(X) = \begin{cases} 0, & \text{if } X \leq 0 \\ 1, & \text{if } X > 0 \end{cases} $
- 可简化为向量形式：$ y = \sigma(\mathbf{w} \cdot \mathbf{x} + b) $，其中$ \mathbf{w} $为权重向量，$ \mathbf{x} $为输入向量，$ b $为偏置。

### 2.2 决策边界
- 决策边界由$ \mathbf{w} \cdot \mathbf{x} + b = 0 $定义，为一个超平面，将输入空间划分为两类。
- 感知机作为线性分类器，只能处理线性可分问题。

### 2.3 局限性
- 感知机无法解决线性不可分问题，如异或（XOR）问题。解决方法是通过多个感知机组合（即多层结构）。

---

## 3. 多层感知机（MLP）

### 3.1 结构
- MLP由输入层、一个或多个隐藏层和输出层组成，各层之间通过权重连接。
- 每层神经元计算输入的加权和加上偏置后，通过激活函数输出。
- 通用逼近定理（Universal Approximation Theorem）：具有足够隐藏层和单元的MLP可以逼近任何有界函数（Hornik et al., 1989）。

### 3.2 激活函数
激活函数引入非线性，常见类型包括：
- **Sigmoid**：$ \sigma(x) = \frac{1}{1 + e^{-x}} $，输出范围(0,1)，常用于分类任务。
- **Tanh**：$ \sigma(x) = \frac{e^{\gamma x} - 1}{e^{\gamma x} + 1} $，输出范围(-1,1)。
- **ReLU (Rectified Linear Unit)**：$ \sigma(x) = \max(0, x) $，加速训练，缓解梯度消失问题。
- **Leaky ReLU**：$ \sigma(x) = \max(ax, x) $，其中$ a \ll 1 $，允许负值梯度流动。

---

## 4. 反向传播算法（Backpropagation）

### 4.1 基本原理
- 反向传播通过迭代处理训练样本，比较预测值与目标值，调整权重以最小化误差函数。
- 误差函数示例：平方和误差$ E = \frac{1}{2} \sum_k (t_k - o_k)^2 $，其中$ t_k $为目标输出，$ o_k $为网络输出。
- 更新方向从输出层向隐藏层逆向传播（因此称为反向传播）。

### 4.2 算法步骤
1. **初始化**：随机初始化权重和偏置。
2. **前向传播**：计算各层神经元的净输入和输出：
   $$
   net_j = \sum_i w_{ji} o_i + b_j, \quad o_j = \sigma(net_j)
   $$
3. **反向传播误差**：计算输出层和隐藏层的误差项$ \delta $。
   - 输出层：$ \delta_k = \sigma'(net_k) \cdot (t_k - o_k) $
   - 隐藏层：$ \delta_j = \sigma'(net_j) \cdot \sum_k \delta_k w_{kj} $
4. **更新权重和偏置**：
   $$
   \Delta w_{ji} = \eta_w \delta_j o_i, \quad \Delta b_j = \eta_b \delta_j
   $$
   其中$ \eta_w $和$ \eta_b $为学习率。
5. **迭代**：重复步骤2-4，直到满足终止条件（如误差低于阈值或达到最大迭代次数）。

### 4.3 梯度下降优化
- **梯度下降**：通过误差函数的梯度更新权重：$ w \leftarrow w - \eta \frac{\partial E}{\partial w} $。
- **学习率$ \eta $**：控制更新步长，过大可能导致发散，过小则收敛慢。
- **变体**：
  - **随机梯度下降（SGD）**：每次随机选择小批量数据更新权重。
  - **案例更新（Case Updating）**：每处理一个样本更新一次。
  - **周期更新（Epoch Updating）**：遍历所有样本后更新。

---

## 5. 卷积神经网络（CNN）

### 5.1 动机与结构
- **问题**：全连接网络在图像处理中参数过多（例如200x200x3图像连接10个神经元需1,200,000个权重），易过拟合。
- **解决方案**：CNN通过权重共享（卷积滤波器）减少参数，专为图像任务设计。
- **典型结构**：卷积层、池化层、全连接层组成。

### 5.2 卷积操作
- **滤波器（Kernel）**：共享权重，用于提取局部特征，生成特征图（Feature Map）。
- **步长（Stride）**：滤波器移动的步长，影响输出尺寸。
- **零填充（Zero Padding）**：在输入边界填充零，调整输入输出尺寸。
- 输出尺寸公式：
  $$
  W_o = \frac{W_i - F + 2P}{S} + 1, \quad H_o = \frac{H_i - F + 2P}{S} + 1
  $$
  其中$ W_i, H_i $为输入宽高，$ F $为滤波器大小，$ P $为填充，$ S $为步长。

### 5.3 池化操作
- **作用**：降采样，减少空间维度和参数，降低过拟合风险。
- **类型**：
  - **最大池化（Max Pooling）**：选择区域内最大值。
  - **平均池化（Average Pooling）**：计算区域平均值。

### 5.4 正则化与过拟合
- **过拟合问题**：参数过多时模型在训练集表现好，但泛化能力差。
- **正则化方法**：
  - **L1/L2正则化**：在损失函数中加入权重惩罚项，如$ R(w) = \sum |w| $或$ R(w) = \sum w^2 $。
  - **提前停止（Early Stopping）**：验证误差不再下降时停止训练。
  - **Dropout**：随机丢弃部分神经元，防止过度依赖特定权重。

### 5.5 著名CNN模型
- **LeNet (1998)**：首个成功应用CNN，5层结构，用于手写数字识别。
- **AlexNet (2012)**：7隐藏层，60M参数，引入ReLU和Dropout，显著提升ImageNet挑战赛成绩。
- **VGG (2014)**：19层，144M参数，强调深度。
- **GoogLeNet (2014)**：22层，引入Inception模块。
- **ResNet (2015)**：引入残差连接，最深152层，解决梯度消失问题。

---

## 6. 循环神经网络（RNN）

### 6.1 结构与特点
- **用途**：处理序列数据（如文本、语音、时间序列）。
- **特点**：各时间步共享参数，通过隐藏状态$ h_t $传递时间依赖：
  $$
  h_t = \sigma(W_h h_{t-1} + W_x x_t)
  $$
  输出：$ y_t = F(h_t) $。

### 6.2 应用示例
- **情感分析**：对文本序列分类（如正面/负面）。
- **图像描述**：结合CNN生成图像描述。

---

## 7. 自编码器（Autoencoder）

### 7.1 结构
- **编码器**：将输入$ X $压缩为潜在空间表示$ Z $。
- **解码器**：从$ Z $重建输入$ X' $。
- **目标**：最小化重建误差$ \|X - X'\| $。
- **瓶颈层（Bottleneck）**：潜在空间维度通常小于输入维度，用于降维。

### 7.2 应用
- **数据压缩**：减少存储需求。
- **去噪**：从噪声数据重建干净数据。
- **图像分割/生成**：用于特征提取或数据增强。

---

## 8. 生成对抗网络（GAN）

### 8.1 结构
- **生成器（Generator）**：生成伪数据。
- **判别器（Discriminator）**：区分真伪数据。
- **训练目标**：通过博弈优化，生成器试图“欺骗”判别器，判别器提高区分能力。

### 8.2 应用
- **图像生成**：如CycleGAN（风格转换）、StyleGAN（高分辨率人脸生成）。
- **数据增强**：生成多样化训练样本。

---

## 9. 变换器（Transformer）

### 9.1 结构
- **编码器-解码器架构**：编码器将输入序列转化为向量表示，解码器逐个生成输出。
- **自注意力机制（Self-Attention）**：通过查询（Q）、键（K）、值（V）计算序列元素间的关系。
- **位置编码（Positional Encoding）**：引入序列中元素的位置信息。

### 9.2 优势与应用
- 相较RNN，训练时间短，适合大规模语言模型（如ChatGPT）。
- 广泛应用于自然语言处理（翻译、文本生成）和图像任务。

---

## 10. 扩散模型（Diffusion Models）

### 10.1 原理
- **前向扩散**：逐步向数据添加噪声。
- **逆向扩散**：从噪声中逐步恢复数据。
- **应用**：如DALL-E 2、Stable Diffusion，用于文本生成图像。

---

## 11. 总结
- 感知机是线性分类器，MLP通过多层结构逼近任意函数。
- CNN专为图像处理设计，通过卷积和池化减少参数。
- RNN适合序列数据处理，Transformer通过自注意力机制提升效率。
- 自编码器用于数据压缩和重建，GAN生成逼真样本。
- 扩散模型为图像生成提供了新方法。

---

## 12. 例题与答案

### 例题1：感知机分类
**问题**：对于逻辑与（AND）和逻辑或（OR）函数，设计感知机的权重和阈值。
- **逻辑与（AND）**：
  - 真值表：(0,0)->0, (0,1)->0, (1,0)->0, (1,1)->1
  - 决策边界：$ x_1 + x_2 - 1.5 = 0 $
  - 权重：$ w_1 = 1, w_2 = 1, b = -1.5 $
- **逻辑或（OR）**：
  - 真值表：(0,0)->0, (0,1)->1, (1,0)->1, (1,1)->1
  - 决策边界：$ x_1 + x_2 - 0.5 = 0 $
  - 权重：$ w_1 = 1, w_2 = 1, b = -0.5 $

**答案**：通过设置合适的权重和偏置，感知机可以实现AND和OR分类。

### 例题2：反向传播计算
**问题**：给定MLP网络，输入$ x = (1,0,1) $，目标输出$ t = 1 $，初始权重和偏置如PPT所示，学习率$ \eta = 0.9 $。计算一次反向传播后的权重更新。
- **前向传播**：
  - 单元4：$ net_4 = 0.2*1 + 0.4*0 + (-0.5)*1 - 0.4 = -0.7, o_4 = 1/(1+e^{0.7}) = 0.332 $
  - 单元5：$ net_5 = (-0.3)*1 + 0.1*0 + 0.2*1 + 0.2 = 0.1, o_5 = 1/(1+e^{-0.1}) = 0.525 $
  - 单元6：$ net_6 = (-0.3)*0.332 + (-0.2)*0.525 + 0.1 = -0.105, o_6 = 1/(1+e^{0.105}) = 0.474 $
- **反向传播误差**：
  - 输出层单元6：$ \delta_6 = o_6 (1-o_6)(t_6 - o_6) = 0.474*(1-0.474)*(1-0.474) = 0.1311 $
  - 隐藏层单元4：$ \delta_4 = o_4 (1-o_4) \delta_6 w_{64} = 0.332*(1-0.332)*0.1311*(-0.3) = -0.0087 $
  - 隐藏层单元5：$ \delta_5 = o_5 (1-o_5) \delta_6 w_{65} = 0.525*(1-0.525)*0.1311*(-0.2) = -0.0065 $
- **权重更新**（部分示例）：
  - $ w_{64} = -0.3 + 0.9*0.1311*0.332 = -0.261 $
  - $ w_{41} = 0.2 + 0.9*(-0.0087)*1 = 0.192 $
  - $ b_6 = 0.1 + 0.9*0.1311 = 0.218 $

**答案**：更新后的权重和偏置见PPT第25页表格。

---
