---
      
title: 11
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---


# 人工智能与数据挖掘：第11周 - 正则化与优化


## 第一部分：课程内容整理

### 1. 复习：聚类算法
#### 1.1 K-Means 聚类
- **输入**：
  - 一组无标签的数据集（无监督学习）。
  - K：聚类数量，作为算法参数需提前指定。
  - 距离度量：本课程中使用欧几里得距离（Euclidean Distance）。
- **初始化**：
  - 各聚类的中心点（Centroids）：初始化的选择会影响最终聚类结果。
- **输出**：
  - 每个数据点的聚类归属（membership）及最终的聚类中心点。
- **终止条件**：
  - 达到预设的迭代次数或中心点不再发生显著变化。

#### 1.2 层次聚类（HAC - Hierarchical Agglomerative Clustering）
- **输入**：
  - 一组无标签的数据集。
  - 距离度量：可以是中心点距离（Centroid Distance）、最小距离（Single Linkage/MIN）、最大距离（Complete Linkage/MAX）或平均距离（Average Linkage）。
- **初始化**：
  - 无需初始化，为确定性算法（Deterministic Algorithm）。
- **操作步骤**：
  - 构建距离表，逐步合并距离最近的两个聚类。
  - 使用树状图（Dendrogram）可视化合并过程。
- **终止条件**：
  - 达到预设的聚类数量K，或直到只剩一个聚类。

---

### 2. 复习：线性回归（Linear Regression）
- **模型定义**：
  - 目标变量 $ y $ 可由权重向量 $ \mathbf{w}^T \mathbf{x} + b $ 确定，其中 $ \mathbf{w} $ 为权重，$ b $ 为偏置，$ \mathbf{x} $ 为输入特征。
  - 为简化表示，将偏置 $ b $ 合并到权重中，即 $ \mathbf{w}^T \gets [\mathbf{w}^T | b] $，$ \mathbf{x} \gets [\mathbf{x} | 1] $，于是模型简化为 $ y = \mathbf{w}^T \mathbf{x} $。
- **训练目标**：
  - 基于训练数据找到最优的权重 $ \mathbf{w} $。
  - 损失函数：均方误差（Mean Squared Error），即：
    $$
    \hat{L}_f(\mathbf{w}) = \frac{1}{N} \sum_{i=1}^N (\mathbf{w}^T \mathbf{x}_i + b - y_i)^2
    $$
    其中 $ N $ 为数据点数量，$ \mathbf{x}_i, y_i $ 为第 $ i $ 个数据点的特征和标签。
- **矩阵形式**：
  - 将所有数据点组合成矩阵 $ \mathbf{X} \in \mathbb{R}^{N \times d} $（$ d $ 为特征维度），标签向量 $ \mathbf{y} \in \mathbb{R}^N $，权重向量 $ \mathbf{w} \in \mathbb{R}^d $。
  - 损失函数改写为：
    $$
    \hat{L}_f(\mathbf{w}) = \frac{1}{N} \|\mathbf{X}\mathbf{w} - \mathbf{y}\|_2^2
    $$
- **梯度下降与闭式解**：
  - 梯度计算：
    $$
    \nabla_{\mathbf{w}} \|\mathbf{X}\mathbf{w} - \mathbf{y}\|_2^2 = 2\mathbf{X}^T\mathbf{X}\mathbf{w} - 2\mathbf{X}^T\mathbf{y}
    $$
  - 将梯度设为零，得到最小化解（闭式解，即最小二乘解）：
    $$
    \mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}
    $$
    （假设 $ \mathbf{X}^T\mathbf{X} $ 可逆）。

---

### 3. 学习与监督
#### 3.1 监督学习与无监督学习的区别
- **监督学习（Supervised Learning）**：
  - 提供带有预定义类别标签的训练数据集，例如分类问题（如图像分类：苹果、梨、牛等）。
  - 目标：通过训练数据学习输入到输出的映射关系。
- **无监督学习（Unsupervised Learning）**：
  - 无标签数据集，例如聚类问题（如K-Means、HAC）。
  - 目标：发现数据中的内在结构或模式。

#### 3.2 监督类型的分类
- **无监督学习**：完全无标签数据。
- **监督学习**：完整、干净的训练标签，适用于目标任务。
- **半监督学习（Semi-Supervised Learning）**：部分训练数据有标签，部分无标签。
- **弱监督学习（Weakly Supervised Learning）**：标签可能有噪声或并非完全针对目标任务。

#### 3.3 监督学习框架
- **训练阶段**：
  - 给定训练集 $ \{(\mathbf{x}_1, y_1), ..., (\mathbf{x}_N, y_N)\} $，学习预测函数 $ y = f_\theta(\mathbf{x}) $ 的参数 $ \theta $。
- **测试阶段**：
  - 对未见过的测试样本 $ \mathbf{x} $，使用学到的模型 $ f_\theta $ 输出预测值 $ y = f_\theta(\mathbf{x}) $。

---

### 4. 学习效果与错误来源
#### 4.1 学习效果的问题
- **过拟合（Overfitting）**：数据量不足或模型过于复杂，导致模型过度拟合训练数据中的噪声，泛化能力差。
- **欠拟合（Underfitting）**：模型过于简单，无法捕捉输入与输出之间的关系。

#### 4.2 学习错误的来源
- **挑战**：
  - 无法确切知道输入到输出的真实映射模型。
  - 无法确切知道数据的真实分布。
- **实际操作**：
  - 对模型作出假设（例如假设分类器为线性）。
  - 使用经验风险最小化（Empirical Risk Minimization）：最小化训练数据上的平均预测误差。

#### 4.3 偏差（Bias）与方差（Variance）
- **偏差**：由于学习算法或模型的错误假设导致的错误。
  - 例如，使用线性回归拟合二次函数会导致高偏差（模型过于简单，欠拟合）。
- **方差**：由于模型对训练数据的小波动敏感而导致的错误。
  - 例如，模型拟合训练数据的噪声会导致高方差（模型过于复杂，过拟合）。
- **期望误差（Expected Error）**：
  - 对于从潜在分布中随机抽取的测试样本，期望被模型 $ f_\theta $ 错误分类的可能性。
  - 期望误差 ≈ 偏差 + 方差 + 噪声。
- **偏差-方差权衡（Bias-Variance Tradeoff）**：
  - 简单模型：高偏差，低方差（易欠拟合）。
  - 复杂模型：低偏差，高方差（易过拟合）。

---

### 5. 统计学习理论基础（可选）
- **为何研究统计学习理论**：
  - 无法确切知道算法在实践中的表现（真实风险），因为无法获取数据的真实分布。
  - 可通过在已知数据集上的表现（经验风险）来近似评估。
- **期望风险（Expected Risk）**：
  - 预测函数 $ h(x) $ 对标签 $ y $ 的预测准确性，通过距离函数 $ l(h(x), y) $ 度量，并根据数据分布 $ p(x, y) $ 加权。
  - 实践中无法直接计算期望风险。
- **经验风险（Empirical Risk）**：
  - 在有限的训练数据集上，计算预测值与真实标签的平均距离。
  - 经验风险近似期望风险，是实际操作中可计算的指标。
- **学习限制与总误差**：
  - 限制1：对模型 $ h(x) \in \mathcal{H} $ 的假设（如线性回归、神经网络）。
  - 限制2：仅能最小化经验风险，而非期望风险。
  - 总学习误差 = 偏差（由限制1导致）+ 方差（由限制2导致）。

---

### 6. 过拟合与欠拟合
- **好模型的定义**：
  - 在偏差与方差之间取得平衡，既不过于简单（欠拟合），也不过于复杂（过拟合）。
- **过拟合的度量**：
  - 过拟合程度 ≈ 测试/验证误差 - 训练误差。
  - 训练误差与测试误差之间的差距越大，过拟合程度越严重。
- **欠拟合与过拟合的表现**：
  - 欠拟合：训练与测试误差差距小，但两者均较高。
  - 过拟合：训练误差低，测试误差高，差距大。

---

### 7. 优化与正则化
#### 7.1 如何监控期望误差
- 从有标签的数据中分离出验证集（Validation Dataset）。
- 在训练集上学习参数，在验证集上评估准确率，模拟测试表现。
- 通过验证集监控过拟合和欠拟合情况。

#### 7.2 模型训练诊断
- **重要统计量**：训练/验证/测试误差曲线。
- **训练参数**：
  - 学习率（Learning Rate）：影响梯度下降的速度。
  - 模型正则化（Regularization）：控制模型复杂度。
  - 迭代次数/轮数（Iterations/Epochs）：影响训练时间与过拟合风险。

#### 7.3 防止过拟合的正则化方法
- **限制模型复杂度**：
  - **Dropout**：训练时随机忽略部分层输出，降低模型对特定神经元的依赖。
  - **Early Stopping**：定期检查验证集误差，当验证误差达到最小值时停止训练，避免训练误差过低导致过拟合。
  - **Weight Sharing**：强制部分神经元参数相同，减少自由参数（如RNN中）。
- **增加训练数据复杂度/数量**：
  - **增加真实数据**：收集更多训练样本。
  - **数据增强（Data Augmentation）**：对现有数据进行变换以增加多样性，如：
    - 几何变换：翻转、旋转、裁剪等。
    - 光度变换：颜色调整。
    - 其他变换：缩放、添加噪声、压缩伪影等。
    - 注意：避免引入明显的人为伪影，需符合数据假设。
- **简化数据分布与维度**：
  - **降维（Dimensionality Reduction）**：减少特征维度，降低模型复杂度。

---

## 第二部分：例题与解答

### 例题1：HAC 层次聚类
**数据点**：
- 点1：(1.9, 1.0)
- 点2：(1.8, 0.9)
- 点3：(2.3, 1.6)
- 点4：(2.3, 2.1)

**问题**：计算不同距离度量下的聚类间距离，并展示合并步骤。
- 距离度量包括：
  1. 单链接（Single Linkage/MIN）：两聚类间的最小距离。
  2. 完备链接（Complete Linkage/MAX）：两聚类间的最大距离。
  3. 中心点距离（Centroid Distance）：两聚类中心点间的距离。
  4. 平均链接（Average Linkage）：两聚类间所有点对的平均距离。

**解答**：
- **初始距离表**（基于欧几里得距离）：
  |      | #1  | #2  | #3  | #4  |
  |------|-----|-----|-----|-----|
  | #1   | 0   | 0.14| 0.72| 1.17|
  | #2   | 0.14| 0   | 0.86| 1.3 |
  | #3   | 0.72| 0.86| 0   | 0.5 |
  | #4   | 1.17| 1.3 | 0.5 | 0   |

- **步骤1**：所有距离度量下，距离最近的为 #1 和 #2（距离=0.14），合并为聚类 #1+#2。

- **步骤2**：计算新聚类 #1+#2 与其他点的距离：
  - **单链接（MIN）**：
    - D(#1+#2, #3) = MIN(D(#1,#3), D(#2,#3)) = MIN(0.72, 0.86) = 0.72
    - D(#1+#2, #4) = MIN(D(#1,#4), D(#2,#4)) = MIN(1.17, 1.3) = 1.17
  - **完备链接（MAX）**：
    - D(#1+#2, #3) = MAX(D(#1,#3), D(#2,#3)) = MAX(0.72, 0.86) = 0.86
    - D(#1+#2, #4) = MAX(D(#1,#4), D(#2,#4)) = MAX(1.17, 1.3) = 1.3
  - **平均链接（Average）**：
    - D(#1+#2, #3) = 0.5 * D(#1,#3) + 0.5 * D(#2,#3) = 0.5*0.72 + 0.5*0.86 = 0.79
    - D(#1+#2, #4) = 0.5 * D(#1,#4) + 0.5 * D(#2,#4) = 0.5*1.17 + 0.5*1.3 = 1.24

- **更新距离表**（以单链接为例，其他类似）：
  |          | #1+#2 | #3  | #4  |
  |----------|-------|-----|-----|
  | #1+#2    | 0     | 0.72| 1.17|
  | #3       | 0.72  | 0   | 0.5 |
  | #4       | 1.17  | 0.5 | 0   |

（后续步骤可继续合并，直至满足终止条件，这里仅展示第一步合并。）

---

### 例题2：线性回归（最小二乘法）
**数据点**：
- (0, 1), y=1
- (1, 1), y=2
- (2, 1), y=3

**问题**：使用最小二乘法求解线性回归模型参数 $ \mathbf{w} $。

**解答**：
- **步骤1**：将数据点扩展为包含偏置项的形式，即 $ \mathbf{x} \gets [\mathbf{x}, 1] $。
  - 数据点：(0, 1), (1, 1), (2, 1)

- **步骤2**：构建数据矩阵 $ \mathbf{X} $ 和标签向量 $ \mathbf{y} $：
  $$
  \mathbf{X} = \begin{bmatrix} 0 & 1 \\ 1 & 1 \\ 2 & 1 \end{bmatrix}, \quad \mathbf{y} = \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix}
  $$

- **步骤3**：计算最小二乘解 $ \mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} $：
  - 计算 $ \mathbf{X}^T\mathbf{X} $：
    $$
    \mathbf{X}^T\mathbf{X} = \begin{bmatrix} 0 & 1 & 2 \\ 1 & 1 & 1 \end{bmatrix} \begin{bmatrix} 0 & 1 \\ 1 & 1 \\ 2 & 1 \end{bmatrix} = \begin{bmatrix} 5 & 3 \\ 3 & 3 \end{bmatrix}
    $$
  - 计算 $ \mathbf{X}^T\mathbf{y} $：
    $$
    \mathbf{X}^T\mathbf{y} = \begin{bmatrix} 0 & 1 & 2 \\ 1 & 1 & 1 \end{bmatrix} \begin{bmatrix} 1 \\ 2 \\ 3 \end{bmatrix} = \begin{bmatrix} 8 \\ 6 \end{bmatrix}
    $$
  - 计算 $ (\mathbf{X}^T\mathbf{X})^{-1} $：
    $$
    (\mathbf{X}^T\mathbf{X})^{-1} = \frac{1}{5*3 - 3*3} \begin{bmatrix} 3 & -3 \\ -3 & 5 \end{bmatrix} = \frac{1}{6} \begin{bmatrix} 3 & -3 \\ -3 & 5 \end{bmatrix}
    $$
  - 计算 $ \mathbf{w} $：
    $$
    \mathbf{w} = \frac{1}{6} \begin{bmatrix} 3 & -3 \\ -3 & 5 \end{bmatrix} \begin{bmatrix} 8 \\ 6 \end{bmatrix} = \begin{bmatrix} 1 \\ 1 \end{bmatrix}
    $$

- **结果**：权重向量 $ \mathbf{w} = [1, 1]^T $，表示模型为 $ y = 1 \cdot x + 1 $。

---

## 总结与复习问题
- **监督类型有哪些？**
  - 无监督学习、监督学习、半监督学习、弱监督学习。
- **如何度量过拟合程度？**
  - 测试/验证误差与训练误差之间的差距，差距越大，过拟合越严重。
- **防止过拟合的方法有哪些？**
  - 降低模型复杂度：Dropout、Early Stopping、Weight Sharing。
  - 增加数据多样性：添加更多训练数据、数据增强。
  - 简化数据分布：降维等方法。

---
