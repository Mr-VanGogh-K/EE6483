---
      
title: 5
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

# 分类与决策树学习文档 (EE6483)

## 1. 引言

### 1.1 分类的定义与应用
分类是一种基于数据样本的特征或属性，将对象分配到预定义类别中的过程。分类在机器学习中是一个核心任务，广泛应用于以下场景：
- 垃圾邮件与非垃圾邮件的分类
- 语音识别
- 图像分类
- 信用卡交易的欺诈与合法性检测

分类的目标是通过学习一个模型（即分类器），将输入数据映射到一个类别标签上。通常表示为：$ y = f(X) $，其中 $ X $ 是输入数据的特征向量，$ y $ 是类别标签。

### 1.2 分类过程的两大步骤
1. **训练（学习）阶段**：基于训练数据集构建分类模型（分类器）。训练数据通常包含已知类别标签的样本，模型通过这些数据学习特征与类别之间的映射关系。
2. **分类（预测）阶段**：利用训练好的分类器对测试数据或未见数据（新数据）的类别标签进行预测。

一个优秀的分类模型应在训练数据上拟合良好，同时在测试数据上具有良好的泛化能力，即对未见数据的预测准确性高。

### 1.3 典型分类流程
分类过程通常包括以下步骤：
1. **原始数据收集**：获取初始数据。
2. **数据预处理或清洗**：处理缺失值、噪声等，提升数据质量。
3. **特征选择**：选择对分类任务最有用的特征，减少维度，提高效率。
4. **数据划分**：将数据分为训练集和测试集。
5. **分类器训练**：利用训练集构建分类模型。
6. **分类器评估**：利用测试集评估模型性能。
7. **新数据预测**：对新数据应用分类器进行预测。

## 2. 数据样本与属性

### 2.1 数据样本的表示
一个数据样本 $ X $ 通常表示为一个 $ n $ 维属性向量：$ X = \{x_1, x_2, \dots, x_n\} $。数据样本也被称为数据点、示例、对象、实例或元组。每个样本 $ X $ 属于一个预定义的类别，由类别标签 $ y $ 指定。类别标签通常是离散值且无序的。

### 2.2 属性类型
属性（特征）是数据样本的描述性变量，根据其性质可分为以下四种类型：
- **标称型 (Nominal)**：用不同的名称区分对象，值之间无序，仅支持相等或不等操作（=，≠）。例如：员工ID号、眼睛颜色、性别、血型。
- **序数型 (Ordinal)**：提供顺序信息，值之间可比较大小（<，>）。例如：评分（差、一般、好、优秀）、街道编号。
- **区间型 (Interval)**：值之间的差值有意义，存在测量单位，支持加减操作（+，-）。例如：日历日期、摄氏或华氏温度。
- **比率型 (Ratio)**：值之间的差值和比值都有意义，支持乘除操作（×，/）。例如：秒数、质量、长度、电流。

## 3. 分类模型概述
分类模型的种类繁多，常见的有：
- 决策树 (Decision Trees)
- 基于规则的分类 (Rule-based Classification)
- K-最近邻分类 (K-Nearest-Neighbor Classification)
- 支持向量机 (Support Vector Machines)
- 神经网络 (Neural Networks)
- 贝叶斯分类器 (Bayesian Classifiers)
- 集成方法 (Ensemble Methods)

本课程主要聚焦于决策树及其相关内容。

## 4. 决策树 (Decision Trees)

### 4.1 决策树结构
决策树是一种类似流程图的树结构，具有以下特点：
- **节点**：每个节点（包括根节点和内部节点）表示对某个属性的测试。
- **分支**：每个分支表示测试的一个结果。
- **叶节点**：每个叶节点（终端节点）包含一个类别标签，表示分类结果。
- **根节点**：树的顶部节点，包含所有训练样本，作为决策树的起点。

决策树通过一系列关于属性的问题对样本进行分类。在二元决策树中，每个非叶节点恰好分裂为两个子节点；而在非二元决策树中，节点可分裂为多个子节点。

### 4.2 决策树构建
决策树的构建通常采用自顶向下的递归方法，基于带类别标签的训练集 $ D $：
1. 从包含所有训练样本的单个根节点 $ N $ 开始。
2. 若节点中的所有样本属于同一类别，则该节点成为叶节点，并标记为该类别。
3. 否则，使用属性选择度量（Attribute Selection Measures）确定哪个属性能够最好地分离样本，选择该属性作为分裂属性。
4. 根据分裂属性的不同取值，将训练集 $ D $ 递归划分为更小的子集。
5. 对每个子集重复上述过程，直到所有样本被唯一分类或无法进一步分裂。

### 4.3 属性选择度量
属性选择度量用于决定哪个属性最适合作为分裂属性，常见的度量包括：
- **信息增益 (Information Gain)**
- **增益比 (Gain Ratio)**
- **基尼指数 (Gini Index)**

#### 4.3.1 信息增益 (Information Gain)
信息增益用于ID3算法，通过最小化分割后的熵（即不确定性或杂质）来选择分裂属性。对于训练集 $ D $，包含 $ m $ 个不同类别的样本，其信息熵定义为：
$$
\text{Info}(D) = -\sum_{i=1}^m p_i \log_2(p_i)
$$
其中，$ p_i = \frac{|C_{i,D}|}{|D|} $ 是样本属于类别 $ c_i $ 的概率，$ |C_{i,D}| $ 是类别 $ c_i $ 的样本数，$ |D| $ 是训练集总样本数。

若属性 $ A $ 将 $ D $ 分割为 $ v $ 个子集 $ \{D_1, D_2, \dots, D_v\} $，则分割后的信息熵为：
$$
\text{Info}_A(D) = \sum_{j=1}^v \frac{|D_j|}{|D|} \times \text{Info}(D_j)
$$
信息增益定义为分割前后信息熵的差值：
$$
\text{Gain}(A) = \text{Info}(D) - \text{Info}_A(D)
$$
选择信息增益最高的属性作为分裂属性。

#### 4.3.2 增益比 (Gain Ratio)
信息增益倾向于选择具有较多类别的属性。为解决这一问题，C4.5算法（ID3的改进版）引入增益比，通过分裂信息对信息增益进行归一化。分裂信息定义为：
$$
\text{SplitInfo}_A(D) = -\sum_{j=1}^v \frac{|D_j|}{|D|} \log_2\left(\frac{|D_j|}{|D|}\right)
$$
增益比定义为：
$$
\text{GainRatio}_A(D) = \frac{\text{Gain}(A)}{\text{SplitInfo}_A(D)}
$$
选择增益比最高的属性作为分裂属性。

#### 4.3.3 基尼指数 (Gini Index)
基尼指数用于CART算法，衡量数据集 $ D $ 的杂质，定义为：
$$
\text{Gini}(D) = 1 - \sum_{i=1}^m p_i^2
$$
对于属性 $ A $ 的二元分割，分割为 $ D_1 $ 和 $ D_2 $，基尼指数为：
$$
\text{Gini}_A(D) = \frac{|D_1|}{|D|} \text{Gini}(D_1) + \frac{|D_2|}{|D|} \text{Gini}(D_2)
$$
基尼指数的减少量为：
$$
\Delta\text{Gini}(A) = \text{Gini}(D) - \text{Gini}_A(D)
$$
选择基尼指数减少量最大的属性或分割点作为分裂属性。对于连续属性，需测试每个可能的分割点，选择基尼指数最小的分割点。

### 4.4 决策树剪枝 (Tree Pruning)
决策树可能因训练数据中的噪声或异常值而过拟合。剪枝技术通过移除不可靠的分支来解决过拟合问题，主要方法有：
- **预剪枝 (Pre-pruning)**：在树构建过程中提前终止，使用叶节点标记当前子集中最常见的类别。
- **后剪枝 (Post-pruning)**：先构建完整的决策树，再移除某些子树，用叶节点替换，并标记为子树中最常见的类别。

### 4.5 决策树的优点
- 构建计算成本低。
- 分类速度快，最坏情况复杂度为 $ O(w) $，其中 $ w $ 是树的最大深度。
- 易于解释。
- 对噪声和冗余属性具有较强的鲁棒性。
- 可避免过拟合问题。
- 对属性的概率分布无特定要求。
- 分类准确性与其他方法相当。

### 4.6 随机森林 (Random Forests)
随机森林是一种集成分类器，利用多个决策树的力量进行决策。其构建方法包括：
- **特征随机性**：每棵决策树基于特征的随机子集构建。
- **Bootstrap聚合 (Bagging)**：每棵决策树在训练集的不同采样（有放回）上训练。
- **多数投票**：最终结果通过各棵树的多数投票决定。

随机森林比单一决策树更准确，能有效处理缺失数据，缓解过拟合问题。

## 5. 分类器性能评估

### 5.1 基本概念与混淆矩阵
分类器的性能通常通过测试集（未用于训练的数据）评估。假设有两类样本：正类 (P) 和负类 (N)，定义如下：
- **真正例 (TP, True Positives)**：正类样本被正确分类为正类。
- **真负例 (TN, True Negatives)**：负类样本被正确分类为负类。
- **假正例 (FP, False Positives)**：负类样本被错误分类为正类。
- **假负例 (FN, False Negatives)**：正类样本被错误分类为负类。

这些指标可通过混淆矩阵表示：

| 实际类别\预测类别 | 是 (Yes) | 否 (No) | 总数 |
|-------------------|----------|----------|------|
| 是 (Yes)          | TP       | FN       | P    |
| 否 (No)           | FP       | TN       | N    |
| 总数              | P'       | N'       | P+N  |

### 5.2 性能度量指标
1. **准确率 (Accuracy)**：
   $$
   \text{accuracy} = \frac{TP + TN}{P + N}
   $$
2. **错误率 (Error Rate)**：
   $$
   \text{error rate} = \frac{FP + FN}{P + N} = 1 - \text{accuracy}
   $$
3. **灵敏度 (Sensitivity, True Positive Rate, Recall)**：
   $$
   \text{sensitivity} = \frac{TP}{P}
   $$
4. **特异度 (Specificity, True Negative Rate)**：
   $$
   \text{specificity} = \frac{TN}{N}
   $$
5. **精确率 (Precision)**：
   $$
   \text{precision} = \frac{TP}{TP + FP}
   $$
6. **F值 (F Measure)**：精确率和召回率的调和平均值：
   $$
   F = \frac{2 \times \text{precision} \times \text{recall}}{\text{precision} + \text{recall}}
   $$
7. **接收者操作特征曲线下的面积 (AUC-ROC)**：衡量分类器在不同阈值下的性能，其中：
   - 真正例率 (TPR) = Sensitivity
   - 假正例率 (FPR) = 1 - Specificity

### 5.3 其他评估维度
- **速度**：构建和应用分类器的计算成本。
- **鲁棒性**：在噪声数据或缺失值数据上的预测能力。
- **可扩展性**：在大规模数据上的构建效率。
- **可解释性**：分类器提供的理解和洞察水平。

### 5.4 数据划分与评估方法
为评估分类器性能，需将数据划分为训练集和测试集，常见方法有：
1. **Holdout方法**：随机将数据分为训练集（通常2/3）和测试集（1/3）。
2. **交叉验证 (Cross-Validation)**：将数据随机分为 $ k $ 个互斥子集（折），每次用一个子集作为测试集，其余作为训练集，重复 $ k $ 次，计算平均性能。
3. **留一法 (Leave-One-Out)**：交叉验证的特例，每次仅留一个样本作为测试集，其余用于训练。

## 6. 总结
- 分类是基于数据属性预测类别的过程，涉及训练和预测两个阶段。
- 数据属性分为标称型、序数型、区间型和比率型。
- 决策树通过一系列属性问题对样本分类，构建时需选择最佳分裂属性。
- 属性选择度量（如信息增益、增益比、基尼指数）用于确定分裂属性。
- 分类器性能通过准确率、灵敏度、特异度、精确率、F值等指标评估。
- 数据划分方法（如Holdout、交叉验证、留一法）用于分离训练与测试数据。

---

## 7. 例题与答案

### 7.1 信息增益计算示例
**数据集**：以下是一个医疗数据集，用于判断患者是否感染流感（Flu）。

| PID | Fever    | Cough | Sore Throat | Tiredness | Flu |
|-----|----------|-------|-------------|-----------|-----|
| 1   | no       | yes   | no          | yes       | −   |
| 2   | no       | yes   | no          | no        | −   |
| 3   | mild     | yes   | no          | yes       | +   |
| 4   | yes      | mild  | no          | yes       | +   |
| 5   | yes      | no    | yes         | yes       | +   |
| 6   | yes      | no    | yes         | no        | −   |
| 7   | mild     | no    | yes         | no        | +   |
| 8   | no       | mild  | no          | yes       | −   |
| 9   | no       | no    | yes         | yes       | +   |
| 10  | yes      | mild  | yes         | yes       | +   |
| 11  | no       | mild  | yes         | no        | +   |
| 12  | mild     | mild  | no          | no        | +   |
| 13  | mild     | yes   | yes         | yes       | +   |
| 14  | yes      | mild  | no          | no        | −   |

**计算**：
- 数据集 $ D $ 的熵：
  $$
  \text{Info}(D) = -\left(\frac{9}{14} \log_2\left(\frac{9}{14}\right) + \frac{5}{14} \log_2\left(\frac{5}{14}\right)\right) = 0.940 \text{ bits}
  $$
- 使用属性“Fever”分割后的熵：
  $$
  \text{Info}_{\text{Fever}}(D) = \frac{5}{14} \times \text{Info}(D_{\text{no}}) + \frac{4}{14} \times \text{Info}(D_{\text{mild}}) + \frac{5}{14} \times \text{Info}(D_{\text{yes}}) = 0.694 \text{ bits}
  $$
- 信息增益：
  $$
  \text{Gain}(\text{Fever}) = 0.940 - 0.694 = 0.246 \text{ bits}
  $$
类似地，计算其他属性的信息增益：
- $\text{Gain}(\text{Cough}) = 0.029 \text{ bits}$
- $\text{Gain}(\text{Sore Throat}) = 0.151 \text{ bits}$
- $\text{Gain}(\text{Tiredness}) = 0.048 \text{ bits}$

**结果**：由于“Fever”具有最高信息增益，因此被选为分裂属性。

### 7.2 增益比计算示例
**数据集**：同上。

**计算**：以属性“Cough”为例：
- 信息增益：$\text{Gain}(\text{Cough}) = 0.029 \text{ bits}$
- 分裂信息：
  $$
  \text{SplitInfo}_{\text{Cough}}(D) = -\left(\frac{4}{14} \log_2\left(\frac{4}{14}\right) + \frac{5}{14} \log_2\left(\frac{5}{14}\right) + \frac{5}{14} \log_2\left(\frac{5}{14}\right)\right) = 1.557 \text{ bits}
  $$
- 增益比：
  $$
  \text{GainRatio}_{\text{Cough}}(D) = \frac{0.029}{1.557} = 0.019
  $$

**结果**：增益比用于选择分裂属性，避免信息增益对多类别属性的偏好。

### 7.3 基尼指数计算示例
**数据集**：同上。

**计算**：
- 数据集 $ D $ 的基尼指数：
  $$
  \text{Gini}(D) = 1 - \left(\frac{9}{14}\right)^2 - \left(\frac{5}{14}\right)^2 = 0.459
  $$
- 以属性“Cough”进行二元分割（“no, mild” vs “yes”）：
  $$
  \text{Gini}_{\text{Cough}}(D) = \frac{10}{14} \text{Gini}(D_1) + \frac{4}{14} \text{Gini}(D_2) = 0.443
  $$

**结果**：需进一步计算其他属性的基尼指数减少量，选取减少量最大的属性作为分裂属性。

### 7.4 分类器性能评估示例
**混淆矩阵**：一个医疗数据集的分类结果如下（流感病毒“是/否”）：

| 实际类别\预测类别 | 是 (Yes) | 否 (No) | 总数   |
|-------------------|----------|---------|--------|
| 是 (Yes)          | 100      | 200     | 300    |
| 否 (No)           | 150      | 9550    | 9700   |
| 总数              | 250      | 9750    | 10000  |

**性能指标**：
- 精确率：$\text{precision} = \frac{100}{250} = 40\%$
- 灵敏度/召回率：$\text{sensitivity/recall} = \frac{100}{300} = 33.33\%$
- 特异度：$\text{specificity} = \frac{9550}{9700} = 98.45\%$
- 准确率：$\text{accuracy} = \frac{100 + 9550}{10000} = 96.50\%$
- F值：$\text{F} = \frac{2 \times 0.4 \times 0.3333}{0.4 + 0.3333} = 36.36\%$

**结果**：上述指标综合反映了分类器的性能，尤其在不平衡数据集中，需关注灵敏度和精确率。
