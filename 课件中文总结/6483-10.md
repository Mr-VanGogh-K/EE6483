---
      
title: 10
      
created: 2025-04-30
      
source: Cherry Studio
      
tags: 
      
---

## 第一部分：无监督学习简介
### 1.1 每周计划概览
- **第10周**：无监督学习 - 聚类与回归
- **第11周**：深度模型的正则化与优化
- **第12周**：贝叶斯推理与降维
- **第13周**：低维度（注：不纳入考试范围）

### 1.2 需要解决的关键问题
- 什么是簇？什么是聚类？
- 聚类与分类的区别是什么？
- K-Means算法的局限性是什么？
- 层次聚类（HAC）算法的局限性是什么？
- 回归与分类的区别是什么？
- 训练线性回归器的损失函数是什么？

---

## 第二部分：聚类
### 2.1 聚类概念
#### 定义
- **簇（Cluster）**：一组彼此相似的数据点。
- **聚类（Clustering）**：一种无监督学习技术，将数据点分组或组织成簇，使得同一簇内的数据点彼此更相似，而与其他簇中的数据点差异更大。

#### 无监督学习
- 与监督学习（如分类）不同，聚类不依赖于预先存在的标签。
- 其目标是发现数据内在的结构或模式。

#### 聚类与分类的区别
- **聚类（无监督）**：
  - 没有预先提供的标签或类别信息。
  - 目标：理解数据的底层结构或组织。
- **分类（监督）**：
  - 训练数据包含标签示例。
  - 目标：学习一个分类器以预测未见过数据的标签。

#### 示例
- 在图像处理中，聚类可用于将代表花朵的像素分组，而无需事先标注；相比之下，分类需要标注的图像来训练模型以检测花朵。

### 2.2 距离度量
#### 距离度量的重要性
- 聚类依赖于衡量数据点之间的“相似性”，通常使用距离度量来量化。
- 给定一组 $ N $ 个数据样本（点） $ \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N $，每个样本表示为一个 $ d $-维向量：
  $$
  \mathbf{x} = \begin{bmatrix} x_1 \\ \vdots \\ x_d \end{bmatrix}
  $$
- 两个数据点 $ \mathbf{x}_i $ 和 $ \mathbf{x}_j $ 之间的距离表示为 $ d(\mathbf{x}_i, \mathbf{x}_j) $，结果为实数标量。

#### 距离度量的性质
有效的距离度量必须满足以下条件：
1. **非负性**：$ d(\mathbf{x}_i, \mathbf{x}_j) \geq 0 $，且 $ d(\mathbf{x}_i, \mathbf{x}_j) = 0 $ 当且仅当 $ \mathbf{x}_i = \mathbf{x}_j $。
2. **对称性**：$ d(\mathbf{x}_i, \mathbf{x}_j) = d(\mathbf{x}_j, \mathbf{x}_i) $。
3. **三角不等式**：$ d(\mathbf{x}_i, \mathbf{x}_j) + d(\mathbf{x}_j, \mathbf{x}_l) \geq d(\mathbf{x}_i, \mathbf{x}_l) $.

#### 常见的距离度量
1. **欧几里得距离**（L2范数）：
   $$
   d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{j=1}^d (x_j - y_j)^2} = \|\mathbf{x} - \mathbf{y}\|_2
   $$
   - 表示两点之间的直线距离。
2. **曼哈顿距离**（L1范数）：
   $$
   d(\mathbf{x}, \mathbf{y}) = \sum_{j=1}^d |x_j - y_j| = \|\mathbf{x} - \mathbf{y}\|_1
   $$
   - 表示每个维度上的绝对差之和。
3. **无穷距离**（L∞范数）：
   $$
   d(\mathbf{x}, \mathbf{y}) = \max_{1 \leq j \leq d} |x_j - y_j|
   $$
   - 表示任意单个维度上的最大差值。

**注**：本课程中通常使用欧几里得距离，除非另有说明。

### 2.3 聚类算法概述
#### 聚类算法的类型
1. **划分算法**：
   - 将数据划分为不重叠的子集（簇）。
   - 每个数据点只属于一个簇。
   - 示例：K-Means、高斯混合模型、光谱聚类。
2. **层次算法**：
   - 将簇组织成层次树结构。
   - 两种方法：
     - **聚合（Agglomerative）**：自底向上，合并最接近的簇。
     - **分裂（Divisive）**：自顶向下，分裂簇。

### 2.4 K-Means聚类
#### 概述
- **目标**：将 $ N $ 个数据点 $ \mathbf{x}_1, \mathbf{x}_2, \dots, \mathbf{x}_N $ 划分为 $ K $ 个簇，每个簇有一个中心点（质心） $ \mu_k $，最小化每个数据点到其分配质心的距离之和。
- **距离度量**：通常为欧几里得距离。
- **质心**：分配到该簇的所有数据点的平均值。

#### 算法步骤
1. **初始化**：随机选择 $ K $ 个点作为初始质心 $ \mu_k $。
2. **迭代**：
   - **分配步骤**：根据距离度量，将每个数据点 $ \mathbf{x}_i $ 分配到最近的质心 $ \mu_k $。
   - **更新步骤**：重新计算每个质心 $ \mu_k $，为其分配数据点的平均值。
3. **停止条件**：当没有数据点更改簇时停止（收敛）。

#### 目标函数
K-Means优化以下代价函数（簇内平方距离和）：
$$
\min_{\mu_k} \min_{r_{ik}} \sum_{i=1}^N \sum_{k=1}^K \frac{1}{2} r_{ik} \|\mathbf{x}_i - \mu_k\|_2^2
$$
约束条件：
- $ r_{ik} \in \{0, 1\} $：指示变量（若点 $ i $ 属于簇 $ k $，则为1，否则为0）。
- $ \sum_{k=1}^K r_{ik} = 1 $：每个点只属于一个簇。
- 质心更新：$ \mu_k = \frac{\sum_{i=1}^N r_{ik} \mathbf{x}_i}{\sum_{i=1}^N r_{ik}} $.

#### 优缺点
- **优点**：
  - 简单且计算效率高。
  - 易于实现。
- **缺点**：
  - 需要预先定义簇的数量 $ K $。
  - 对初始化敏感；可能收敛到较差的局部最小值。
  - 需要合适的距离度量（如对非线性可分数据表现不佳）。
  - 容易受到异常值的影响。

#### 初始化的影响
- 较差的初始化可能导致次优聚类结果（局部最小值）。
- 良好的初始化策略（如K-Means++）可以改善结果。

### 2.5 层次聚合聚类（HAC）
#### 概述
- **方法**：自底向上的聚类，每个数据点开始时作为一个单独的簇，迭代合并最近的簇对，直到仅剩一个（或 $ K $ 个）簇。
- **可视化**：合并过程通常表示为树状图（Dendrogram），显示合并序列。

#### 算法步骤
1. **初始化**：将每个数据点视为一个单独的簇。
2. **迭代**：基于距离度量合并两个最近的簇。
3. **停止条件**：当所有点都在一个簇中或剩余 $ K $ 个簇时停止。

#### 簇间距离度量
1. **单链接（Single Linkage, MIN）**：两个簇中任意点对之间的最小距离。
2. **完全链接（Complete Linkage, MAX）**：两个簇中任意点对之间的最大距离。
3. **平均链接（Average Linkage）**：两个簇中所有点对的平均距离。
4. **质心距离（Centroid Distance）**：两个簇质心（均值）之间的距离。

#### 优缺点
- **优点**：
  - 不需要预先定义簇的数量 $ K $。
  - 确定性（结果无随机性）。
  - 树状图允许通过在相应层次“切割”选择 $ K $。
- **缺点**：
  - 计算复杂度高（需要计算所有簇对之间的距离）。
  - 内存占用大（需存储树状图）。

#### 与K-Means的比较
- **K-Means**：简单、快速，但对初始化敏感且需预定义 $ K $。
- **HAC**：确定性、$ K $ 选择灵活，但计算成本更高。

---

## 第三部分：回归
### 3.1 回归概念
#### 定义
- **回归**：一种监督学习技术，用于从输入数据预测连续输出（数量）。
- **与分类的对比**：
  - **分类**：预测离散类别标签（如图像中是否有猫）。
  - **回归**：预测连续值（如事件的可能性或价格等数值）。
  - **准确性度量**：
    - 分类：正确分类示例的百分比。
    - 回归：均方根误差（RMSE）或类似的连续预测误差度量。

#### 示例
- 预测图像中猫的可能性的连续值（如0.9213），而不是二元标签（是/否）。

### 3.2 线性回归
#### 概述
- **目标**：从 $ d $-维输入向量 $ \mathbf{x} \in \mathbb{R}^d $ 预测标量输出 $ y \in \mathbb{R} $，使用线性模型：
  $$
  y = \mathbf{w}^T \mathbf{x} + b
  $$
  其中 $ \mathbf{w} \in \mathbb{R}^d $ 为权重向量，$ b $ 为偏置项。
- **训练**：使用训练数据集 $ \{(\mathbf{x}_i, y_i)\}_{i=1}^N $ 找到最优的 $ \mathbf{w} $ 和 $ b $。

#### 损失函数
- 目标是最小化均方误差（MSE）：
  $$
  \min_{\mathbf{w}, b} \hat{L}(\mathbf{w}, b) = \frac{1}{N} \sum_{i=1}^N (\mathbf{w}^T \mathbf{x}_i + b - y_i)^2
  $$
  - 这是预测值与实际值之间垂直偏移（平方误差）的最小化。

#### 推导（最小二乘解）
1. **矩阵形式**：
   - 将偏置纳入，增强 $ \mathbf{x}_i \leftarrow \begin{bmatrix} \mathbf{x}_i \\ 1 \end{bmatrix} $，$ \mathbf{w} \leftarrow \begin{bmatrix} \mathbf{w} \\ b \end{bmatrix} $。
   - 将所有输入堆叠成矩阵 $ \mathbf{X} \in \mathbb{R}^{N \times (d+1)} $，输出为向量 $ \mathbf{y} \in \mathbb{R}^N $。
   - 损失函数变为：
     $$
     \hat{L}(\mathbf{w}) = \frac{1}{N} \|\mathbf{X}\mathbf{w} - \mathbf{y}\|_2^2
     $$
2. **梯度计算**：
   - 关于 $ \mathbf{w} $ 的梯度：
     $$
     \nabla_{\mathbf{w}} \|\mathbf{X}\mathbf{w} - \mathbf{y}\|_2^2 = 2\mathbf{X}^T\mathbf{X}\mathbf{w} - 2\mathbf{X}^T\mathbf{y}
     $$
3. **梯度置零**：
   - 求解最小化器：
     $$
     \mathbf{X}^T\mathbf{X}\mathbf{w} = \mathbf{X}^T\mathbf{y}
     $$
     $$
     \mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T\mathbf{y}
     $$
   - 这是**最小二乘（LS）估计器**，假设 $ \mathbf{X}^T\mathbf{X} $ 可逆。

#### 局限性与扩展
- **线性局限性**：线性模型可能无法拟合复杂的非线性关系。
- **解决方法**：使用特征变换 $ \phi(\mathbf{x}_i) $（如多项式特征），将输入映射到更高维空间，再应用线性回归。

---

## 第四部分：示例与解答
### 4.1 K-Means聚类示例
#### 示例1：2D数据聚类（$ K=3 $）
- **数据点**：A1(2,10), A2(2,5), A3(8,4), B1(5,8), B2(7,5), B3(6,4), C1(1,2), C2(4,9)
- **初始质心**：A1(2,10), B1(5,8), C1(1,2)
- **距离度量**：欧几里得距离
- **步骤**：
  1. **首次分配**（基于到初始质心的距离）：
     - 簇1：{A1}
     - 簇2：{B1, B2, B3, A3, C2}
     - 簇3：{A2, C1}
  2. **更新质心**（分配点的平均值）：
     - C1：(2, 10)
     - C2：(6, 6)
     - C3：(1.5, 3.5)
  3. **重复分配与更新**直到无变化。
- **注**：最终簇取决于迭代次数（PPT未完全计算）。

#### 示例2：视觉示例（PPT幻灯片39-40）
- 视觉展示质心在迭代中逐渐收敛到簇中心。

#### 示例3：图像像素聚类（PPT幻灯片41-42）
- **1D特征**：聚类像素灰度强度（$ K=2, K=3 $）。
- **3D特征**：聚类像素RGB颜色值（$ K=3 $）。
- **目的**：基于强度或颜色相似性将图像分割为区域。

### 4.2 HAC示例
#### 示例：4个点聚类（PPT幻灯片67-76）
- **数据点**：#1(1.9,1.0), #2(1.8,0.9), #3(2.3,1.6), #4(2.3,2.1)
- **距离度量**：
  1. **质心距离**：
     - 初始距离矩阵：
       - (#1, #2)=0.14, (#3, #4)=0.5，等。
     - 首次合并：#1和#2（距离=0.14），新质心=(1.85,0.95)
     - 更新距离，下次合并：#3和#4（距离=0.5）
     - 最终合并为一个簇。
  2. **平均链接**：
     - 初始合并：#1和#2（距离=0.14）
     - 更新距离：D(1+2,3)=0.79, D(1+2,4)=1.24
     - 下次合并：#3和#4（距离=0.5）
     - 最终距离：D(1+2,3+4)=1.02
- **结果**：树状图显示合并顺序和距离。

### 4.3 线性回归示例
#### 示例：1D数据回归（PPT幻灯片88-89）
- **数据**：$ x $-$ y $平面上的点。
- **目标**：拟合直线 $ y = \mathbf{w}^T \mathbf{x} + b $，最小化垂直平方误差。
- **可视化**：显示拟合直线及预测值与实际值的平方误差项（垂直偏移）。

---

## 第五部分：关键要点总结
### 聚类
- **定义**：无标签情况下将相似数据点分组（无监督学习）。
- **距离度量**：定义相似性的基本工具（例如欧几里得距离、曼哈顿距离）。
- **K-Means**：简单的划分方法，最小化簇内方差，但对初始化敏感。
- **HAC**：层次合并，确定性，$ K $ 选择灵活，但计算成本高。

### 回归
- **定义**：预测连续输出（监督学习）。
- **线性回归**：通过最小化均方误差（最小二乘解）拟合线性模型。
- **局限性**：对非线性关系表现不佳，可通过特征变换解决。

---

## 第六部分：关键问题的答案
1. **什么是簇？什么是聚类？**
   - **簇**是一组彼此相似的数据点。
   - **聚类**是将数据点分组的过程，使得同一组内的数据点彼此更相似，与其他组的数据点差异更大。
2. **聚类与分类的区别是什么？**
   - 聚类是无监督的（无标签），目标是发现数据结构。
   - 分类是有监督的（使用标签），目标是预测类别标签。
3. **K-Means算法的局限性是什么？**
   - 需要预先定义簇的数量 $ K $。
   - 对初始化敏感（可能收敛到较差的局部最小值）。
   - 需要合适的距离度量（对非线性可分数据表现不佳）。
4. **HAC算法的局限性是什么？**
   - 计算和内存开销大，因需计算所有簇对的距离并存储树状图。
5. **回归与分类的区别是什么？**
   - 回归预测连续量。
   - 分类预测离散类别标签。
6. **训练线性回归器的损失函数是什么？**
   - 均方误差（MSE）：$ \frac{1}{N} \sum_{i=1}^N (\mathbf{w}^T \mathbf{x}_i + b - y_i)^2 $，最小化垂直偏移。

---
